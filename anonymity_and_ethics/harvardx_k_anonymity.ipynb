{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "761d2176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = \"/Users/andrewrodriguez/Desktop/compsci1050/anonymity_and_ethics/reduced_qi_filled.csv\"\n",
    "OUT_RECORD_SUPP = \"/Users/andrewrodriguez/Desktop/compsci1050/anonymity_and_ethics/de-identified_data/record_suppression.csv\"\n",
    "OUT_COLUMN_SUPP = \"/Users/andrewrodriguez/Desktop/compsci1050/anonymity_and_ethics/de-identified_data/column_suppression.csv\"\n",
    "OUT_GEN = \"/Users/andrewrodriguez/Desktop/compsci1050/anonymity_and_ethics/de-identified_data/generalized.csv\"\n",
    "OUT_COMBINATION = \"/Users/andrewrodriguez/Desktop/compsci1050/anonymity_and_ethics/de-identified_data/combination.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feebaf3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 199999\n",
      "Columns: ['course_id', 'user_id', 'cc_by_ip', 'city', 'postalCode', 'LoE', 'YoB', 'gender', 'nforum_posts', 'nforum_votes', 'nforum_endorsed', 'nforum_threads', 'nforum_comments', 'nforum_pinned', 'nforum_events']\n",
      "(199999, 15)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List\n",
    "\n",
    "# Read data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Columns:\", list(df.columns))\n",
    "df.head(3)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f58deeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identifiers: ['user_id', 'course_id']\n",
      "QI columns (13): ['cc_by_ip', 'city', 'postalCode', 'LoE', 'YoB', 'gender', 'nforum_posts', 'nforum_votes', 'nforum_endorsed', 'nforum_threads', 'nforum_comments', 'nforum_pinned', 'nforum_events']\n"
     ]
    }
   ],
   "source": [
    "# so user_id and course_id are idenitifiers here\n",
    "identifiers = [\"user_id\", \"course_id\"]\n",
    "qi_cols = [c for c in df.columns if c not in identifiers]\n",
    "\n",
    "print(\"Identifiers:\", identifiers)\n",
    "print(\"QI columns ({}):\".format(len(qi_cols)), qi_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a01fdb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline k-anonymity: 1\n"
     ]
    }
   ],
   "source": [
    "# Okay lets check our baseline k anon\n",
    "# to do so im gonna make a helper\n",
    "from typing import List, Tuple\n",
    "\n",
    "def k_anonymity_level(data: pd.DataFrame, qis: List[str]) -> int:\n",
    "    # So the level of k anonmyity is data grouped by quasi ids\n",
    "    return int(data.groupby(qis, dropna=False).size().min())\n",
    "\n",
    "k0 = k_anonymity_level(df, qi_cols)\n",
    "print(\"Baseline k-anonymity:\", k0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f2adb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_suppression_k(data: pd.DataFrame, qis: List[str], k: int = 5) -> Tuple[pd.DataFrame, int]:\n",
    "    # Okay so for record supression we only keep rows of size k\n",
    "    # where k is 5\n",
    "    sizes = data.groupby(qis, dropna=False).size().rename(\"size\")\n",
    "    tmp = data.join(sizes, on=qis)\n",
    "    kept = tmp[tmp[\"size\"] >= k].drop(columns=[\"size\"])\n",
    "    deleted = len(tmp) - len(kept)\n",
    "    return kept, deleted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ef071ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rows_source': 199999, 'deleted_rows': 150286, 'rows_kept': 49713, 'k_after': 5, 'file': '/Users/andrewrodriguez/Desktop/compsci1050/anonymity_and_ethics/de-identified_data/record_suppression.csv'}\n"
     ]
    }
   ],
   "source": [
    "# Now we want to record supress to k = 5\n",
    "rs_df, rs_deleted = record_suppression_k(df, qi_cols, k=5)\n",
    "rs_k = k_anonymity_level(rs_df, qi_cols)\n",
    "rs_df.to_csv(OUT_RECORD_SUPP, index=False)\n",
    "\n",
    "print({\n",
    "    \"rows_source\": len(df),\n",
    "    \"deleted_rows\": rs_deleted,\n",
    "    \"rows_kept\": len(rs_df),\n",
    "    \"k_after\": rs_k,\n",
    "    \"file\": OUT_RECORD_SUPP\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "778c8dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with just column cc_by_ip k = 1\n",
      "with just column city k = 1\n",
      "with just column postalCode k = 1\n",
      "with just column LoE k = 535\n",
      "with just column YoB k = 1\n",
      "with just column gender k = 978\n",
      "with just column nforum_posts k = 1\n",
      "with just column nforum_votes k = 1\n",
      "with just column nforum_endorsed k = 1\n",
      "with just column nforum_threads k = 1\n",
      "with just column nforum_comments k = 1\n",
      "with just column nforum_pinned k = 1\n",
      "with just column nforum_events k = 1\n"
     ]
    }
   ],
   "source": [
    "# this is the check for the column supression I mentioned in the report\n",
    "for q in qi_cols:\n",
    "    print(\"with just column\", q, \"k =\", k_anonymity_level(df, q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "274f4e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_column_suppression_until_k(data, identifiers, target_k=5, count_na_as_value=True, verbose=False):\n",
    "    # Start with all QIs\n",
    "    current_qis = [c for c in data.columns if c not in identifiers]\n",
    "    dropped = []\n",
    "\n",
    "    # Current k\n",
    "    current_k = k_anonymity_level(data, current_qis)\n",
    "    if verbose:\n",
    "        print(f\"Start: k={current_k}, QIs={len(current_qis)}\")\n",
    "\n",
    "    # How to count uniques (count NaN as its own value or not)\n",
    "    nunique_kwargs = {\"dropna\": not count_na_as_value}\n",
    "\n",
    "    # Precompute per-column cardinalities (they donâ€™t change as we drop other columns)\n",
    "    cardinality = {c: data[c].nunique(**nunique_kwargs) for c in current_qis}\n",
    "\n",
    "    while current_k < target_k and current_qis:\n",
    "        # Find the maximum cardinality among remaining QIs\n",
    "        max_card = max(cardinality[c] for c in current_qis)\n",
    "        candidates = [c for c in current_qis if cardinality[c] == max_card]\n",
    "\n",
    "        # Tie-break among candidates: pick the one whose removal yields the highest k,\n",
    "        # then the fewest groups, then lexicographically.\n",
    "        best_col = None\n",
    "        best_k = -1\n",
    "        best_groups = None\n",
    "        best_name = None\n",
    "\n",
    "        for c in candidates:\n",
    "            trial_qis = [x for x in current_qis if x != c]\n",
    "            kval = k_anonymity_level(data, trial_qis)\n",
    "            ng = (data.groupby(trial_qis, dropna=False).ngroups if trial_qis else 1)\n",
    "\n",
    "            if (kval > best_k) or \\\n",
    "               (kval == best_k and (best_groups is None or ng < best_groups)) or \\\n",
    "               (kval == best_k and ng == best_groups and (best_name is None or c < best_name)):\n",
    "                best_k = kval\n",
    "                best_groups = ng\n",
    "                best_col = c\n",
    "                best_name = c\n",
    "\n",
    "        # Drop the chosen column\n",
    "        current_qis.remove(best_col)\n",
    "        dropped.append(best_col)\n",
    "        current_k = best_k\n",
    "        cardinality.pop(best_col, None)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Dropped '{best_col}' (card={max_card}) -> k={current_k}, QIs left={len(current_qis)}\")\n",
    "\n",
    "    return dropped, current_k, current_qis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ea94c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: k=1, QIs=13\n",
      "Dropped 'postalCode' (card=18489) -> k=1, QIs left=12\n",
      "Dropped 'city' (card=13276) -> k=1, QIs left=11\n",
      "Dropped 'nforum_events' (card=645) -> k=1, QIs left=10\n",
      "Dropped 'cc_by_ip' (card=218) -> k=1, QIs left=9\n",
      "Dropped 'YoB' (card=124) -> k=1, QIs left=8\n",
      "Dropped 'nforum_posts' (card=110) -> k=1, QIs left=7\n",
      "Dropped 'nforum_comments' (card=104) -> k=1, QIs left=6\n",
      "Dropped 'nforum_votes' (card=91) -> k=1, QIs left=5\n",
      "Dropped 'nforum_threads' (card=55) -> k=1, QIs left=4\n",
      "Dropped 'LoE' (card=12) -> k=1, QIs left=3\n",
      "Dropped 'nforum_endorsed' (card=11) -> k=1, QIs left=2\n",
      "Dropped 'nforum_pinned' (card=9) -> k=978, QIs left=1\n"
     ]
    }
   ],
   "source": [
    "dropped_cols, final_k, remaining_qis = greedy_column_suppression_until_k(\n",
    "    df, identifiers, target_k=5, count_na_as_value=True, verbose=True\n",
    ")\n",
    "\n",
    "# Save if you hit k >= 5\n",
    "if final_k >= 5:\n",
    "    keep_cols = identifiers + remaining_qis\n",
    "    cs_df = df[keep_cols].copy()\n",
    "    cs_df.to_csv(OUT_COLUMN_SUPP, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "336b710f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== configure \"high\" threshold for forum stats here =====\n",
    "HIGH_INTERACTION_THRESHOLD = 1  # 1 if value >= 21, else 0\n",
    "\n",
    "\n",
    "def postal_to_binary(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Map postal codes by their first digit:\n",
    "      - '0'â€“'4' -> 0\n",
    "      - '5'â€“'9' -> 1\n",
    "      - missing/invalid -> 'Unknown'\n",
    "    \"\"\"\n",
    "    vals = s.astype(str).str.strip()\n",
    "    out = []\n",
    "    for v in vals:\n",
    "        if v == \"\" or v.lower() in {\"nan\",\"none\",\"null\",\"na\"}:\n",
    "            out.append(1); continue\n",
    "        m = re.search(r\"\\d\", v)\n",
    "        if not m:\n",
    "            out.append(1); continue\n",
    "        d = int(m.group(0))\n",
    "        out.append(0 if d <= 4 else 1)\n",
    "    return pd.Series(out, index=s.index, dtype=\"object\")\n",
    "\n",
    "\n",
    "# Hemisphere lookup: assign countries to N or S hemisphere\n",
    "_NORTH = {\"US\",\"CA\",\"MX\",\"GL\",\"GB\",\"IE\",\"FR\",\"DE\",\"ES\",\"IT\",\"NL\",\"BE\",\"LU\",\"PT\",\"SE\",\"NO\",\"DK\",\"FI\",\"IS\",\n",
    "          \"PL\",\"CZ\",\"SK\",\"HU\",\"AT\",\"CH\",\"SI\",\"HR\",\"BA\",\"RS\",\"ME\",\"MK\",\"AL\",\"GR\",\"RO\",\"BG\",\"LT\",\"LV\",\"EE\",\n",
    "          \"UA\",\"MD\",\"BY\",\"CN\",\"JP\",\"KR\",\"TW\",\"HK\",\"MO\",\"IN\",\"PK\",\"BD\",\"LK\",\"NP\",\"BT\",\"MV\",\"SG\",\"MY\",\"TH\",\n",
    "          \"VN\",\"PH\",\"ID\",\"KH\",\"LA\",\"MM\",\"MN\",\"KZ\",\"UZ\",\"TM\",\"TJ\",\"KG\",\"AE\",\"SA\",\"IR\",\"IQ\",\"IL\",\"JO\",\"LB\",\n",
    "          \"SY\",\"TR\",\"YE\",\"OM\",\"QA\",\"KW\",\"BH\",\"AM\",\"AZ\",\"GE\",\"ZA\",\"EG\",\"NG\",\"ET\",\"KE\",\"TZ\",\"DZ\",\"MA\",\"TN\",\n",
    "          \"GH\",\"CI\",\"CM\",\"UG\",\"SD\",\"SN\",\"RW\",\"ZM\",\"ZW\",\"MW\",\"MZ\",\"AO\",\"NA\",\"BW\",\"GA\",\"GM\",\"GN\",\"GW\",\"LS\",\n",
    "          \"LR\",\"LY\",\"ML\",\"MR\",\"NE\",\"SC\",\"SL\",\"SO\",\"SS\",\"TD\",\"TG\",\"BI\",\"BJ\",\"BF\",\"CD\",\"CG\",\"CV\",\"DJ\",\"ER\",\n",
    "          \"GQ\",\"KM\",\"MG\",\"MU\",\"RE\",\"SH\",\"ST\",\"SZ\",\"AU\",\"NZ\"}  # very rough cut, adjust as needed\n",
    "\n",
    "def to_hemisphere(s: pd.Series) -> pd.Series:\n",
    "    vals = s.astype(str).str.strip()\n",
    "    out = []\n",
    "    for v in vals:\n",
    "        vv = v.upper()\n",
    "        if vv in _NORTH:\n",
    "            out.append(\"North\")\n",
    "        elif vv == \"\" or vv in {\"NAN\",\"NONE\",\"UNKNOWN\"}:\n",
    "            out.append(\"North\")\n",
    "        else:\n",
    "            out.append(\"South\")\n",
    "    return pd.Series(out, index=vals.index, dtype=\"object\")\n",
    "\n",
    "# Representative \"largest\" city per hemisphere\n",
    "_HEMISPHERE_LARGEST_CITY = {\n",
    "    \"North\": \"Tokyo\",     # largest in northern hemisphere\n",
    "    \"South\": \"SÃ£o Paulo\", # largest in southern hemisphere\n",
    "}\n",
    "\n",
    "\n",
    "# ---- helpers ----\n",
    "def birth_to_pre1990_plus(s: pd.Series) -> pd.Series:\n",
    "    s_str = s.astype(\"string\").str.strip()\n",
    "    years_num = pd.to_numeric(s_str, errors=\"coerce\")\n",
    "    years_dt = pd.to_datetime(s_str, errors=\"coerce\")\n",
    "    years = years_num.fillna(years_dt.dt.year)\n",
    "\n",
    "    out = pd.Series(\"2000+\", index=s.index, dtype=\"object\")\n",
    "    mask_unknown = years.isna()\n",
    "    mask_pre = (years < 2000).fillna(False)\n",
    "    out[mask_pre] = \"pre-2000\"\n",
    "    out[mask_unknown] = \"pre-2000\"\n",
    "    return out\n",
    "\n",
    "# keep if you still need categorical bins elsewhere\n",
    "def counts_to_categories(x: pd.Series) -> pd.Series:\n",
    "    xi = pd.to_numeric(x, errors=\"coerce\")\n",
    "    bins   = [-np.inf, 0, 5, 20, np.inf]\n",
    "    labels = [\"0\", \"1-5\", \"6-20\", \"21+\"]\n",
    "    return pd.cut(xi.fillna(-np.inf), bins=bins, labels=labels, include_lowest=True).astype(str)\n",
    "\n",
    "# NEW: binary high/low for interaction-like columns\n",
    "def counts_to_binary(x: pd.Series, threshold: int = HIGH_INTERACTION_THRESHOLD) -> pd.Series:\n",
    "    xi = pd.to_numeric(x, errors=\"coerce\").fillna(0)\n",
    "    return (xi >= threshold).astype(int)\n",
    "\n",
    "def loe_to_binary(s: pd.Series) -> pd.Series:\n",
    "    v = s.astype(\"string\").str.strip().str.lower()\n",
    "    v = v.replace({\"\": pd.NA, \"nan\": pd.NA, \"none\": pd.NA, \"null\": pd.NA, \"unknown\": pd.NA})\n",
    "    def map_loe(t):\n",
    "        if t is None or t is pd.NA: return \"â‰¤Secondary\"\n",
    "        if any(k in t for k in [\"less than\",\"primary\",\"elementary\",\"middle\",\"secondary\",\"high school\",\"hs\"]):\n",
    "            return \"â‰¤Secondary\"\n",
    "        if \"associate\" in t: return \"Tertiary+\"\n",
    "        if any(k in t for k in [\"bachelor\",\"college\",\"undergrad\"]): return \"Tertiary+\"\n",
    "        if any(k in t for k in [\"master\",\"graduate\",\"professional\"]): return \"Tertiary+\"\n",
    "        if any(k in t for k in [\"doctor\",\"doctoral\",\"phd\",\"dphil\",\"md\",\"jd\"]): return \"Tertiary+\"\n",
    "        return \"â‰¤Secondary\"\n",
    "    return v.map(map_loe)\n",
    "\n",
    "\n",
    "def postal_first_digit_or_00000(s: pd.Series) -> pd.Series:\n",
    "    vals = s.astype(str)\n",
    "    out = []\n",
    "    for v in vals:\n",
    "        v_strip = v.strip()\n",
    "        if v_strip == \"\" or v_strip.lower() in {\"nan\", \"none\", \"null\", \"na\"}:\n",
    "            out.append(\"00000\"); continue\n",
    "        m = re.search(r\"\\d\", v_strip)\n",
    "        out.append(m.group(0) + \"0000\" if m else \"00000\")\n",
    "    return pd.Series(out, index=s.index, dtype=\"object\")\n",
    "\n",
    "# ---- single generalization pass ----\n",
    "def generalize(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    One-pass generalization:\n",
    "      - Birth (year or full date) -> {'pre-1990','1990+','Unknown'}\n",
    "      - Country/region -> continent (NA/EU/AS/AF/OC/SA/Unknown)\n",
    "      - City -> largest city label for that continent\n",
    "      - Postal code -> first digit or '00000' (as 'D0000')\n",
    "      - Level of Education -> binary ('â‰¤Secondary' vs 'Tertiary+')\n",
    "      - Interaction/counter fields -> BINARY high (>= {thr})=1, else 0\n",
    "      - Gender -> normalize: any 'o'->'m', NaN->'f'\n",
    "      - Other columns unchanged\n",
    "    \"\"\"\n",
    "    g = data.copy()\n",
    "    lower = {c.lower(): c for c in g.columns}\n",
    "\n",
    "    # Birth â†’ pre-1990 / 1990+\n",
    "    for key in (\"date of birth\",\"dob\",\"birthdate\",\"birth date\",\"year of birth\",\"yob\",\"ear of birth\"):\n",
    "        if key in lower:\n",
    "            col = lower[key]\n",
    "            g[col] = birth_to_pre1990_plus(g[col])\n",
    "            break\n",
    "\n",
    "\n",
    "    # Country/region â†’ hemisphere\n",
    "    hemi_col = None\n",
    "    for key in (\"cc_by_ip\",\"country_by_ip\",\"country\",\"region\"):\n",
    "        if key in lower:\n",
    "            hemi_col = lower[key]\n",
    "            g[hemi_col] = to_hemisphere(g[hemi_col])\n",
    "            break\n",
    "\n",
    "    # City â†’ largest city in that hemisphere\n",
    "    if \"city\" in lower:\n",
    "        city_col = lower[\"city\"]\n",
    "        if hemi_col is not None:\n",
    "            g[city_col] = g[hemi_col].map(_HEMISPHERE_LARGEST_CITY).fillna(\"UnknownCity\")\n",
    "        else:\n",
    "            g[city_col] = \"UnknownCity\"\n",
    "\n",
    "    # Postal code â†’ first digit or 00000\n",
    "    for key in (\"postal_code\",\"postalcode\",\"postal code\",\"zip\",\"zip_code\"):\n",
    "        if key in lower:\n",
    "            pcol = lower[key]\n",
    "            g[pcol] = postal_to_binary(g[pcol])\n",
    "            break\n",
    "\n",
    "    # Level of Education â†’ binary\n",
    "    for key in (\"level of education\",\"loe\",\"education_level\",\"education level\"):\n",
    "        if key in lower:\n",
    "            loe_col = lower[key]\n",
    "            g[loe_col] = loe_to_binary(g[loe_col])\n",
    "            break\n",
    "\n",
    "    # Interaction / counter fields â†’ BINARY\n",
    "    tokens = (\"nforum\",\"number of \",\"events within the forum\",\"interactions\",\n",
    "              \"posts\",\"comments\",\"views\",\"votes\",\"clicks\",\"plays\",\"nevents\",\"nplay\")\n",
    "    for c in g.columns:\n",
    "        name = c.lower()\n",
    "        if any(t in name for t in tokens):\n",
    "            try:\n",
    "                g[c] = counts_to_binary(g[c])  # 1 if >= threshold, else 0\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    # Gender normalization: any 'o' -> 'm', NaN -> 'f'\n",
    "    if \"gender\" in lower:\n",
    "        col = lower[\"gender\"]\n",
    "        s = g[col].astype(\"string\").str.strip().str.lower()\n",
    "        s = s.replace({\"\": pd.NA, \"nan\": pd.NA, \"none\": pd.NA, \"null\": pd.NA})\n",
    "        s = s.replace({\"o\": \"m\", \"male\": \"m\", \"m\": \"m\", \"female\": \"f\", \"f\": \"f\"})\n",
    "        g[col] = s.fillna(\"f\")\n",
    "\n",
    "    return g\n",
    "\n",
    "# (optional) k utility\n",
    "def k_anonymity_level(df: pd.DataFrame, qis: List[str]) -> int:\n",
    "    if not qis:\n",
    "        return len(df)\n",
    "    sizes = df.groupby(qis, dropna=False).size()\n",
    "    return int(sizes.min()) if len(sizes) else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e42b0b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_4/ykqt5y3x1g9g_q8_17d7jbhm0000gn/T/ipykernel_60157/56986269.py:59: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  years_dt = pd.to_datetime(s_str, errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k after full generalization: 1\n"
     ]
    }
   ],
   "source": [
    "gdf = generalize(df)\n",
    "gdf.to_csv(OUT_GEN, index=False)\n",
    "print(\"k after full generalization:\", k_anonymity_level(gdf, qi_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "66d16d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of k=1 groups: 41\n",
      "Violating records shape: (41, 16)\n",
      "                      course_id   user_id cc_by_ip       city postalCode  \\\n",
      "3324    HarvardX/ER22.1x/1T2018  14527936    North      Tokyo          0   \n",
      "5471    HarvardX/HKS101A/2015T3   3958359    South  SÃ£o Paulo          0   \n",
      "5813    HarvardX/HKS101A/2015T3   6494823    North      Tokyo          0   \n",
      "6107    HarvardX/HKS101A/2015T3   7577470    North      Tokyo          1   \n",
      "8997  HarvardX/PH201x/2013_SOND   1130567    North      Tokyo          1   \n",
      "\n",
      "             LoE       YoB gender  nforum_posts  nforum_votes  \\\n",
      "3324  â‰¤Secondary  pre-2000      m             1             1   \n",
      "5471  â‰¤Secondary  pre-2000      m             1             1   \n",
      "5813  â‰¤Secondary     2000+      m             1             0   \n",
      "6107  â‰¤Secondary  pre-2000      f             1             1   \n",
      "8997  â‰¤Secondary  pre-2000      f             1             1   \n",
      "\n",
      "      nforum_endorsed  nforum_threads  nforum_comments  nforum_pinned  \\\n",
      "3324                1               1                1              1   \n",
      "5471                0               0                1              0   \n",
      "5813                0               1                0              0   \n",
      "6107                0               1                1              1   \n",
      "8997                0               1                1              1   \n",
      "\n",
      "      nforum_events  count  \n",
      "3324              0      1  \n",
      "5471              0      1  \n",
      "5813              0      1  \n",
      "6107              0      1  \n",
      "8997              1      1  \n"
     ]
    }
   ],
   "source": [
    "# Find group sizes for the QI columns\n",
    "sizes = gdf.groupby(qi_cols, dropna=False).size()\n",
    "\n",
    "# Mask for groups with size = 1\n",
    "rare_groups = sizes[sizes == 1]\n",
    "\n",
    "print(\"Number of k=1 groups:\", len(rare_groups))\n",
    "\n",
    "# Get the actual records that belong to those groups\n",
    "violating_records = gdf.merge(\n",
    "    rare_groups.rename(\"count\"),\n",
    "    how=\"inner\",\n",
    "    left_on=qi_cols,\n",
    "    right_index=True\n",
    ")\n",
    "\n",
    "print(\"Violating records shape:\", violating_records.shape)\n",
    "print(violating_records.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9cb90d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cc_by_ip</th>\n",
       "      <th>city</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>LoE</th>\n",
       "      <th>YoB</th>\n",
       "      <th>gender</th>\n",
       "      <th>nforum_posts</th>\n",
       "      <th>nforum_votes</th>\n",
       "      <th>nforum_endorsed</th>\n",
       "      <th>nforum_threads</th>\n",
       "      <th>nforum_comments</th>\n",
       "      <th>nforum_pinned</th>\n",
       "      <th>nforum_events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HarvardX/PH525.1x/1T2018</td>\n",
       "      <td>29940</td>\n",
       "      <td>North</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>1</td>\n",
       "      <td>â‰¤Secondary</td>\n",
       "      <td>pre-2000</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HarvardX/PH525.1x/1T2018</td>\n",
       "      <td>37095</td>\n",
       "      <td>North</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>1</td>\n",
       "      <td>â‰¤Secondary</td>\n",
       "      <td>pre-2000</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HarvardX/PH525.1x/1T2018</td>\n",
       "      <td>45634</td>\n",
       "      <td>South</td>\n",
       "      <td>SÃ£o Paulo</td>\n",
       "      <td>1</td>\n",
       "      <td>â‰¤Secondary</td>\n",
       "      <td>pre-2000</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HarvardX/PH525.1x/1T2018</td>\n",
       "      <td>52234</td>\n",
       "      <td>North</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>1</td>\n",
       "      <td>â‰¤Secondary</td>\n",
       "      <td>pre-2000</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HarvardX/PH525.1x/1T2018</td>\n",
       "      <td>52238</td>\n",
       "      <td>North</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>1</td>\n",
       "      <td>â‰¤Secondary</td>\n",
       "      <td>pre-2000</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  course_id  user_id cc_by_ip       city postalCode  \\\n",
       "0  HarvardX/PH525.1x/1T2018    29940    North      Tokyo          1   \n",
       "1  HarvardX/PH525.1x/1T2018    37095    North      Tokyo          1   \n",
       "2  HarvardX/PH525.1x/1T2018    45634    South  SÃ£o Paulo          1   \n",
       "3  HarvardX/PH525.1x/1T2018    52234    North      Tokyo          1   \n",
       "4  HarvardX/PH525.1x/1T2018    52238    North      Tokyo          1   \n",
       "\n",
       "          LoE       YoB gender  nforum_posts  nforum_votes  nforum_endorsed  \\\n",
       "0  â‰¤Secondary  pre-2000      f             0             0                0   \n",
       "1  â‰¤Secondary  pre-2000      m             0             0                0   \n",
       "2  â‰¤Secondary  pre-2000      m             0             0                0   \n",
       "3  â‰¤Secondary  pre-2000      m             0             0                0   \n",
       "4  â‰¤Secondary  pre-2000      f             0             0                0   \n",
       "\n",
       "   nforum_threads  nforum_comments  nforum_pinned  nforum_events  \n",
       "0               0                0              0              0  \n",
       "1               0                0              0              0  \n",
       "2               0                0              0              0  \n",
       "3               0                0              0              0  \n",
       "4               0                0              0              0  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f698b1d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n",
      "5\n",
      "(199842, 15)\n"
     ]
    }
   ],
   "source": [
    "record_supressed_gdf, deleted = record_suppression_k(gdf, qi_cols)\n",
    "print(deleted)\n",
    "print(k_anonymity_level(record_supressed_gdf, qi_cols))\n",
    "print(record_supressed_gdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d9339c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    199999.0\n",
       "mean          1.0\n",
       "std           0.0\n",
       "min           1.0\n",
       "25%           1.0\n",
       "50%           1.0\n",
       "75%           1.0\n",
       "max           1.0\n",
       "Name: nforum_posts, dtype: float64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf[\"nforum_posts\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8cf14a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>course_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cc_by_ip</th>\n",
       "      <th>city</th>\n",
       "      <th>postalCode</th>\n",
       "      <th>LoE</th>\n",
       "      <th>YoB</th>\n",
       "      <th>gender</th>\n",
       "      <th>nforum_posts</th>\n",
       "      <th>nforum_votes</th>\n",
       "      <th>nforum_endorsed</th>\n",
       "      <th>nforum_threads</th>\n",
       "      <th>nforum_comments</th>\n",
       "      <th>nforum_pinned</th>\n",
       "      <th>nforum_events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HarvardX/PH525.1x/1T2018</td>\n",
       "      <td>29940</td>\n",
       "      <td>NA</td>\n",
       "      <td>Mexico City</td>\n",
       "      <td>70000</td>\n",
       "      <td>â‰¤Secondary</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HarvardX/PH525.1x/1T2018</td>\n",
       "      <td>37095</td>\n",
       "      <td>AS</td>\n",
       "      <td>Tokyo</td>\n",
       "      <td>00000</td>\n",
       "      <td>â‰¤Secondary</td>\n",
       "      <td>1990+</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HarvardX/PH525.1x/1T2018</td>\n",
       "      <td>45634</td>\n",
       "      <td>SA</td>\n",
       "      <td>Sao Paulo</td>\n",
       "      <td>00000</td>\n",
       "      <td>â‰¤Secondary</td>\n",
       "      <td>pre-1990</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HarvardX/PH525.1x/1T2018</td>\n",
       "      <td>52234</td>\n",
       "      <td>EU</td>\n",
       "      <td>Moscow</td>\n",
       "      <td>00000</td>\n",
       "      <td>â‰¤Secondary</td>\n",
       "      <td>pre-1990</td>\n",
       "      <td>m</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HarvardX/PH525.1x/1T2018</td>\n",
       "      <td>52238</td>\n",
       "      <td>NA</td>\n",
       "      <td>Mexico City</td>\n",
       "      <td>00000</td>\n",
       "      <td>â‰¤Secondary</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  course_id  user_id cc_by_ip         city postalCode  \\\n",
       "0  HarvardX/PH525.1x/1T2018    29940       NA  Mexico City      70000   \n",
       "1  HarvardX/PH525.1x/1T2018    37095       AS        Tokyo      00000   \n",
       "2  HarvardX/PH525.1x/1T2018    45634       SA    Sao Paulo      00000   \n",
       "3  HarvardX/PH525.1x/1T2018    52234       EU       Moscow      00000   \n",
       "4  HarvardX/PH525.1x/1T2018    52238       NA  Mexico City      00000   \n",
       "\n",
       "          LoE       YoB gender  nforum_posts  nforum_votes  nforum_endorsed  \\\n",
       "0  â‰¤Secondary   Unknown      f             0             0                0   \n",
       "1  â‰¤Secondary     1990+      m             0             0                0   \n",
       "2  â‰¤Secondary  pre-1990      m             0             0                0   \n",
       "3  â‰¤Secondary  pre-1990      m             0             0                0   \n",
       "4  â‰¤Secondary   Unknown      f             0             0                0   \n",
       "\n",
       "   nforum_threads  nforum_comments  nforum_pinned  nforum_events  \n",
       "0               0                0              0              0  \n",
       "1               0                0              0              0  \n",
       "2               0                0              0              0  \n",
       "3               0                0              0              0  \n",
       "4               0                0              0              0  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "786c4f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rows_source': 199999, 'deleted_rows': 68207, 'rows_kept': 131792, 'k_after': 5, 'file': '/Users/andrewrodriguez/Desktop/compsci1050/anonymity_and_ethics/de-identified_data/combination.csv'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Choose Stage 1 if it helps; otherwise Stage 2\n",
    "base_gen = g1 if g1_k >= 5 else g2\n",
    "base_qis = [c for c in base_gen.columns if c not in identifiers]\n",
    "combo_df, combo_deleted = record_suppression_k(base_gen, base_qis, k=5)\n",
    "combo_k = k_anonymity_level(combo_df, base_qis)\n",
    "combo_df.to_csv(OUT_COMBINATION, index=False)\n",
    "\n",
    "print({\n",
    "    \"rows_source\": len(df),\n",
    "    \"deleted_rows\": combo_deleted,\n",
    "    \"rows_kept\": len(combo_df),\n",
    "    \"k_after\": combo_k,\n",
    "    \"file\": OUT_COMBINATION\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
